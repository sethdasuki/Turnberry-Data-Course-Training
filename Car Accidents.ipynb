#Import libraries

import pandas as pd

#Read car accidents file

accidents_file_path = 'US_Accidents_March23.csv'

#Read data in chunks to reduce memory
chunk_size = 100000

accidents = pd.DataFrame()

for chunk in pd.read_csv(accidents_file_path,chunksize=chunk_size, low_memory=False):
    # Process each chunk as needed
    accidents = pd.concat([accidents,chunk])
    
print(len(accidents))


#Create sample from accidents data

sample = accidents.sample(n=100000, random_state = 1000)
print(sample)

export = sample.to_csv('C:/Users/SethDasuki/OneDrive - Turnberry Solutions Inc/Desktop/Sample_Accidents 100,000.csv')
